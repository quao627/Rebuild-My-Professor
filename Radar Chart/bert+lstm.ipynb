{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert+lstm.ipynb","provenance":[],"collapsed_sections":["W01OfgAQnlu_","apyGTC7z0O_x","IkoI_5k30isS","vKAgUt-w0o3-","OkrCpaK_DJtE"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8dc053c0b38e4bd5864d6c912a8e1a6f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f382d5c0910e4677a1c48816c86101bb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ba9e4852aa9247e19b4d472a7cdc1e6e","IPY_MODEL_1c76e3695b45470ca044fbf1f0703d78"]}},"f382d5c0910e4677a1c48816c86101bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ba9e4852aa9247e19b4d472a7cdc1e6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5239ea1da0c04991a1e86915cd348066","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e8d6ee87ebfc4af386ec2c64e2bbb8ab"}},"1c76e3695b45470ca044fbf1f0703d78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c6715d543bc24c4797b9d8d2172527c0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:01&lt;00:00, 150kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5233a8a97b814ed0abda0a9802ae6eaf"}},"5239ea1da0c04991a1e86915cd348066":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e8d6ee87ebfc4af386ec2c64e2bbb8ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6715d543bc24c4797b9d8d2172527c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5233a8a97b814ed0abda0a9802ae6eaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c41401de3bf4ef1a66f94f530935f03":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5ea1351d27c64c268d5552341ac8a1c2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7f01f36afa274198b8325d7dc31b1fe1","IPY_MODEL_108ad405b5f94e739778d2e3b81733b4"]}},"5ea1351d27c64c268d5552341ac8a1c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7f01f36afa274198b8325d7dc31b1fe1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d13267dcecc540d383da2f78e7f7f6df","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9d18a6be59c74426a7adcaf7d693a74c"}},"108ad405b5f94e739778d2e3b81733b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_77df028f163f49be8ff1e2fa7b8dbf9b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 59.1B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_588ce7ede79f4dd9a7e567cbc652b5e2"}},"d13267dcecc540d383da2f78e7f7f6df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9d18a6be59c74426a7adcaf7d693a74c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77df028f163f49be8ff1e2fa7b8dbf9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"588ce7ede79f4dd9a7e567cbc652b5e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"493877481b0745c994f084b129e68161":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d086d8c868974c8e996259600616db89","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fbfbf51af1e9478ba0b51700203316f4","IPY_MODEL_c8e52efd32cd4cbda786a24e6fc314cc"]}},"d086d8c868974c8e996259600616db89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fbfbf51af1e9478ba0b51700203316f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_af4e3995cf214a219e1f36eb9b05ec76","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_244d265ed06a439e87e6a3a799e52e98"}},"c8e52efd32cd4cbda786a24e6fc314cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9173b865cfe44f1aae88b1e827c2773c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 1.37MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_385c6a5e39bd4c3bb77af5cee5ab3b5a"}},"af4e3995cf214a219e1f36eb9b05ec76":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"244d265ed06a439e87e6a3a799e52e98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9173b865cfe44f1aae88b1e827c2773c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"385c6a5e39bd4c3bb77af5cee5ab3b5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d8a3d850fec44285aef0258412a3a2b5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_457da33fc4c744f7a25177f0593b61e7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_92388c76c90d4d04adfb364939568c3a","IPY_MODEL_0f995ffaff5945608c2b391debfc7f88"]}},"457da33fc4c744f7a25177f0593b61e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"92388c76c90d4d04adfb364939568c3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_485dc69c9c42491abf758845fbf2a805","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b2d2f7c190014879b1ab9b8799f1fd1f"}},"0f995ffaff5945608c2b391debfc7f88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6ddb437c0cda4680bad9ae4c0e7612e2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 1.74kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1765cd2449424d7a8b7cb753f21fdd89"}},"485dc69c9c42491abf758845fbf2a805":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b2d2f7c190014879b1ab9b8799f1fd1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6ddb437c0cda4680bad9ae4c0e7612e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1765cd2449424d7a8b7cb753f21fdd89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"deeaeacff72f4092828a20590d527f99":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_102a6507d19145b7bfe2c50fa4343512","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_402f74db896d45dc9665085f85357dc0","IPY_MODEL_31965475e2f845bfa5754d29bb440701"]}},"102a6507d19145b7bfe2c50fa4343512":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"402f74db896d45dc9665085f85357dc0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_de13a24719314a2d8b74ac3a3c0b7ac5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_44bb573cdec445eb9a77c2983679f898"}},"31965475e2f845bfa5754d29bb440701":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c1531c0cf1614bff9f525bb77ae822c1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:08&lt;00:00, 50.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c5d87c70ede947ec81e2ce1f1941f42a"}},"de13a24719314a2d8b74ac3a3c0b7ac5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"44bb573cdec445eb9a77c2983679f898":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c1531c0cf1614bff9f525bb77ae822c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c5d87c70ede947ec81e2ce1f1941f42a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"775b41a07481493c92196f68c488b106":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4156eb1ee72e4b049cd4002a39ae19e8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fd967ed2d9bd47359bff7b07b7c52fca","IPY_MODEL_e2538205981b4ee4955c9393c7413d17"]}},"4156eb1ee72e4b049cd4002a39ae19e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd967ed2d9bd47359bff7b07b7c52fca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5648a19bb57c4c43a581753074c5e94a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":442,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_438a8ca8c60b4889a064350f4f7b4380"}},"e2538205981b4ee4955c9393c7413d17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_023a937b4df34132ba786543722810e0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 442/442 [00:00&lt;00:00, 9.99kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cfa609f12c98430aa57324315116a5f0"}},"5648a19bb57c4c43a581753074c5e94a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"438a8ca8c60b4889a064350f4f7b4380":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"023a937b4df34132ba786543722810e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cfa609f12c98430aa57324315116a5f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f60d1b0774fb4574af40f96489198c69":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4d3d6ba7a7b34e5ba36927fe5096bf3d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5bb4aa0365694f9c891e07b76af8d73f","IPY_MODEL_5d2bf2f2c71f4ab0ba73756225acd2b9"]}},"4d3d6ba7a7b34e5ba36927fe5096bf3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5bb4aa0365694f9c891e07b76af8d73f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e468a8de4221487ea760afc0f5029dd3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4758bd89546e436694983727f3058bf8"}},"5d2bf2f2c71f4ab0ba73756225acd2b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_730a501b60824df5823575ea3bc3c28b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [00:05&lt;00:00, 49.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac603a6c43ac48d59d7b8b9b591c484d"}},"e468a8de4221487ea760afc0f5029dd3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4758bd89546e436694983727f3058bf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"730a501b60824df5823575ea3bc3c28b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ac603a6c43ac48d59d7b8b9b591c484d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"fA2dGJipvUaY"},"source":["# Fine Tuning Transformer for MultiClass Text Classification\n","- Script Objective: The objective of this script is to fine tune BERT to be able to classify a news headline into the following categories:\n","  0. Others\n","  1. Participation matters\n","  2. have extra credit\n","  3. engaging lecture\n","  4. helpful office hour\n","  5. heavy workload\n"]},{"cell_type":"markdown","metadata":{"id":"O0Idye2FjUBC"},"source":["# GPU Prep"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8zeeiG0IvfqR","executionInfo":{"status":"ok","timestamp":1618085469629,"user_tz":300,"elapsed":6793,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"e129efe3-4d08-4d8e-8a8e-a1beac69ea58"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s5bzljLYvxup","executionInfo":{"status":"ok","timestamp":1618085473290,"user_tz":300,"elapsed":9221,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"d215a606-43d2-4b2f-83c7-02918f3ce74f"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mT4hZ6TTlKoC"},"source":["# Import Libraries"]},{"cell_type":"code","metadata":{"id":"OtyPOf6fv0tU"},"source":["!pip3 install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5fNsb41ko84"},"source":["# Importing the libraries needed\n","import torch\n","import transformers\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertModel, BertTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6dOc_AjQjGZg"},"source":["# Data Prep"]},{"cell_type":"code","metadata":{"id":"glb_WLPBjWPL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618085551100,"user_tz":300,"elapsed":79154,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"63822f3c-8a5e-4b6b-86f7-50a8017d70b6"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive\n","\n","%matplotlib inline\n","drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S-NpxwR8kQ3P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617950365563,"user_tz":300,"elapsed":490,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"187931ec-9330-4ab2-d369-2246d7aaf274"},"source":["# Read data\n","input_path = \"/content/drive/My Drive/Rebuild my Professor/Clusters_xue_Mar7/\"\n","participation = pd.read_csv(input_path + \"participation2_final.csv\", names=[\"id\", \"sentence\", \"cat\"], header=0)\n","bonus = pd.read_csv(input_path + \"a23-Extra Credit.csv\", names=[\"id\", \"sentence\", \"cat\"], header=0)\n","lecture = pd.read_csv(input_path + \"a12 - Funny Lectures.csv\", names=[\"id\", \"sentence\", \"cat\"], header=0)\n","office_hour = pd.read_csv(input_path + \"a3 - helpful office hour.csv\", names=[\"id\", \"sentence\", \"cat\"], header=0) \n","other = pd.read_excel(input_path + \"other.xlsx\", names=[\"id\", \"sentence\", \"cat\"], header=0)\n","workload = pd.read_excel(input_path + \"workload.xlsx\", names=[\"id\", \"sentence\", \"cat\"], header=0)\n","\n","\n","# Remove unlabeled data\n","participation = participation[:243]\n","participation[\"cat\"] = participation[\"cat\"].fillna(0)\n","participation.loc[participation[\"cat\"] == 1, \"cat\"] = 1\n","\n","bonus[\"cat\"] = bonus[\"cat\"].fillna(0)\n","bonus.loc[bonus[\"cat\"] == 1, \"cat\"] = 2\n","\n","lecture[\"cat\"] = lecture[\"cat\"].fillna(0)\n","lecture.loc[lecture[\"cat\"] == 2, \"cat\"] = 3\n","\n","office_hour = office_hour[:180]\n","office_hour[\"cat\"] = office_hour[\"cat\"].fillna(0)\n","office_hour.loc[office_hour[\"cat\"] == 3, \"cat\"] = 4\n","\n","workload[\"cat\"] = workload[\"cat\"].fillna(0)\n","workload.loc[workload[\"cat\"] == 1, \"cat\"] = 5\n","\n","other = other[:200]\n","other.loc[other[\"cat\"] == 1, \"cat\"] = 0\n","other = other.loc[other[\"cat\"] == 0, :]\n","\n","\n","data = pd.concat([participation, bonus, lecture, office_hour, workload, other]).reset_index().drop([\"index\", \"id\"], axis=1)\n","data[\"cat\"].value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0    448\n","3.0    288\n","2.0    185\n","4.0    157\n","1.0    127\n","5.0    125\n","Name: cat, dtype: int64"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"WEqQeC-IhlMC","executionInfo":{"status":"ok","timestamp":1617950366519,"user_tz":300,"elapsed":296,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"ddb151a9-3768-4433-f917-a488e4e8809c"},"source":["data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>cat</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>As long as you do the work, participate in cla...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Summers is the man, and as long as you show up...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>For tests, just know the review guide and whic...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Come to class and you will be fine.</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Classes were lecture heavy - study them and yo...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence  cat\n","0  As long as you do the work, participate in cla...  1.0\n","1  Summers is the man, and as long as you show up...  1.0\n","2  For tests, just know the review guide and whic...  0.0\n","3                Come to class and you will be fine.  1.0\n","4  Classes were lecture heavy - study them and yo...  1.0"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"rJIXnF4bkv9R"},"source":["## Dataloader and Dataset Prep"]},{"cell_type":"code","metadata":{"id":"YBHO-Y94kt24","colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["8dc053c0b38e4bd5864d6c912a8e1a6f","f382d5c0910e4677a1c48816c86101bb","ba9e4852aa9247e19b4d472a7cdc1e6e","1c76e3695b45470ca044fbf1f0703d78","5239ea1da0c04991a1e86915cd348066","e8d6ee87ebfc4af386ec2c64e2bbb8ab","c6715d543bc24c4797b9d8d2172527c0","5233a8a97b814ed0abda0a9802ae6eaf","6c41401de3bf4ef1a66f94f530935f03","5ea1351d27c64c268d5552341ac8a1c2","7f01f36afa274198b8325d7dc31b1fe1","108ad405b5f94e739778d2e3b81733b4","d13267dcecc540d383da2f78e7f7f6df","9d18a6be59c74426a7adcaf7d693a74c","77df028f163f49be8ff1e2fa7b8dbf9b","588ce7ede79f4dd9a7e567cbc652b5e2","493877481b0745c994f084b129e68161","d086d8c868974c8e996259600616db89","fbfbf51af1e9478ba0b51700203316f4","c8e52efd32cd4cbda786a24e6fc314cc","af4e3995cf214a219e1f36eb9b05ec76","244d265ed06a439e87e6a3a799e52e98","9173b865cfe44f1aae88b1e827c2773c","385c6a5e39bd4c3bb77af5cee5ab3b5a"]},"executionInfo":{"status":"ok","timestamp":1618086292072,"user_tz":300,"elapsed":2686,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"d337c6f1-474e-431e-f412-f953019c25ce"},"source":["# Defining some key variables that will be used later on in the training\n","MAX_LEN = 64\n","TRAIN_BATCH_SIZE = 6\n","VALID_BATCH_SIZE = 6\n","EPOCHS = 8\n","LEARNING_RATE = 8e-05\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8dc053c0b38e4bd5864d6c912a8e1a6f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c41401de3bf4ef1a66f94f530935f03","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"493877481b0745c994f084b129e68161","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hzWoeaRDlNiN"},"source":["class Triage(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        \n","    def __getitem__(self, index):\n","        sentence = str(self.data.sentence[index])\n","        sentence = \" \".join(sentence.split())\n","        inputs = self.tokenizer.encode_plus(\n","            sentence,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True,\n","            truncation=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'targets': torch.tensor(self.data.cat[index], dtype=torch.long)\n","        } \n","    \n","    def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJ64VOYBw7gS","executionInfo":{"status":"ok","timestamp":1617950371889,"user_tz":300,"elapsed":562,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"80e9625d-1781-4aa9-ddd7-83ca6cf2a10e"},"source":["# Creating the dataset and dataloader for the neural network\n","\n","train_size = 0.8\n","\n","train_dataset = data.sample(frac=train_size,random_state=2000)\n","validate_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(data.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"VALIDATE Dataset: {}\".format(validate_dataset.shape))\n","\n","training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n","validating_set = Triage(validate_dataset, tokenizer, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FULL Dataset: (1330, 2)\n","TRAIN Dataset: (1064, 2)\n","VALIDATE Dataset: (266, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iOPQk8c7lQtZ"},"source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","validate_params = {'batch_size': VALID_BATCH_SIZE,\n","                   'shuffle': False,\n","                   'num_workers': 0\n","                   }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","validating_loader = DataLoader(validating_set, **validate_params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OQhVKtNDlR42"},"source":["# Build the Model"]},{"cell_type":"code","metadata":{"id":"c9LNxTLEmPtu"},"source":["import torch.nn as nn\n","\n","class BertLSTMClass(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_dim,\n","                 output_dim,\n","                 n_layers,\n","                 bidirectional,\n","                 dropout):\n","        \n","        super().__init__()\n","        \n","        self.bert = bert\n","        \n","        embedding_dim = bert.config.to_dict()['hidden_size']\n","\n","        self.rnn = nn.GRU(embedding_dim,\n","                          hidden_dim,\n","                          num_layers = n_layers,\n","                          bidirectional = bidirectional,\n","                          batch_first = True,\n","                          dropout = 0 if n_layers < 2 else dropout)\n","        \n","        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","\n","    def forward(self, input_ids, attention_mask):\n","        \n","        #text = [batch size, sent len]\n","                \n","        with torch.no_grad():\n","            embedded = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]\n","                \n","        #embedded = [batch size, sent len, emb dim]\n","        \n","        _, hidden = self.rnn(embedded)\n","        \n","        #hidden = [n layers * n directions, batch size, emb dim]\n","        \n","        if self.rnn.bidirectional:\n","            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","        else:\n","            hidden = self.dropout(hidden[-1,:,:])\n","                \n","        #hidden = [batch size, hid dim]\n","        \n","        output = self.out(hidden)\n","        \n","        #output = [batch size, out dim]\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2QMda-Imn0d","colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["d8a3d850fec44285aef0258412a3a2b5","457da33fc4c744f7a25177f0593b61e7","92388c76c90d4d04adfb364939568c3a","0f995ffaff5945608c2b391debfc7f88","485dc69c9c42491abf758845fbf2a805","b2d2f7c190014879b1ab9b8799f1fd1f","6ddb437c0cda4680bad9ae4c0e7612e2","1765cd2449424d7a8b7cb753f21fdd89","deeaeacff72f4092828a20590d527f99","102a6507d19145b7bfe2c50fa4343512","402f74db896d45dc9665085f85357dc0","31965475e2f845bfa5754d29bb440701","de13a24719314a2d8b74ac3a3c0b7ac5","44bb573cdec445eb9a77c2983679f898","c1531c0cf1614bff9f525bb77ae822c1","c5d87c70ede947ec81e2ce1f1941f42a"]},"executionInfo":{"status":"ok","timestamp":1617945202022,"user_tz":300,"elapsed":12075,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"f10710f3-00b3-4cc2-b3ac-d7a04a2fc347"},"source":["HIDDEN_DIM = 12\n","OUTPUT_DIM = 6\n","N_LAYERS = 2\n","BIDIRECTIONAL = True\n","DROPOUT = 0.6\n","\n","bert = BertModel.from_pretrained('bert-base-uncased')\n","model = BertLSTMClass(bert,\n","                      HIDDEN_DIM,\n","                      OUTPUT_DIM,\n","                      N_LAYERS,\n","                      BIDIRECTIONAL,\n","                      DROPOUT)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8a3d850fec44285aef0258412a3a2b5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"deeaeacff72f4092828a20590d527f99","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCWAGJ2Nm84o","executionInfo":{"status":"ok","timestamp":1617911645146,"user_tz":300,"elapsed":217,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"c5feaa56-c48c-435b-b102-f8aca8c5f620"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 109,541,430 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HCrUnQQfndRr"},"source":["for name, param in model.named_parameters():                \n","    if name.startswith('bert'):\n","        param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-ybMwsUngO6","executionInfo":{"status":"ok","timestamp":1617911647821,"user_tz":300,"elapsed":216,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"56ee1a25-f782-4a5d-c3ef-e1b6b23e2cf8"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 59,190 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLZNp6POnh3w","executionInfo":{"status":"ok","timestamp":1617911651233,"user_tz":300,"elapsed":327,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"d7156ec2-40cc-4b84-ccbd-4e07e5f37715"},"source":["for name, param in model.named_parameters():                \n","    if param.requires_grad:\n","        print(name)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["rnn.weight_ih_l0\n","rnn.weight_hh_l0\n","rnn.bias_ih_l0\n","rnn.bias_hh_l0\n","rnn.weight_ih_l0_reverse\n","rnn.weight_hh_l0_reverse\n","rnn.bias_ih_l0_reverse\n","rnn.bias_hh_l0_reverse\n","rnn.weight_ih_l1\n","rnn.weight_hh_l1\n","rnn.bias_ih_l1\n","rnn.bias_hh_l1\n","rnn.weight_ih_l1_reverse\n","rnn.weight_hh_l1_reverse\n","rnn.bias_ih_l1_reverse\n","rnn.bias_hh_l1_reverse\n","out.weight\n","out.bias\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W01OfgAQnlu_"},"source":["# Train the Model"]},{"cell_type":"code","metadata":{"id":"MN_8-Xo4nlQc"},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBpiS-HenpGG"},"source":["criterion = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NrsqCUZn_Iz"},"source":["model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZN1TyFrPoDlh"},"source":["# Function to calcuate the accuracy of the model\n","def calcuate_accu(big_idx, targets):\n","    n_correct = (big_idx==targets).sum().item()\n","    return n_correct"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6w8R7LWYoJyN"},"source":["def train(model, iterator, optimizer, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.train()\n","    \n","    for batch in iterator:\n","        \n","        optimizer.zero_grad()\n","\n","        ids = batch['ids'].to(device, dtype = torch.long)\n","        mask = batch['mask'].to(device, dtype = torch.long)\n","        targets = batch['targets'].to(device, dtype = torch.long)\n","\n","        predictions = model(ids, mask)\n","        loss = criterion(predictions, targets)\n","        \n","        big_val, big_idx = torch.max(predictions.data, dim=1)\n","        acc = calcuate_accu(big_idx, targets) / TRAIN_BATCH_SIZE\n","        \n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wDq6PDvoOJC"},"source":["def evaluate(model, iterator, criterion):\n","    \n","    epoch_loss = 0\n","    epoch_acc = 0\n","    \n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for batch in iterator:\n","\n","            ids = batch['ids'].to(device, dtype = torch.long)\n","            mask = batch['mask'].to(device, dtype = torch.long)\n","            targets = batch['targets'].to(device, dtype = torch.long)\n","\n","            predictions = model(ids, mask).squeeze()\n","            loss = criterion(predictions, targets)\n","            \n","            big_val, big_idx = torch.max(predictions.data, dim=1)\n","            acc = calcuate_accu(big_idx, targets) / TRAIN_BATCH_SIZE\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc\n","        \n","    return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Wv6Yi8VoO0K"},"source":["import time\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eF7oYn8LoP9z","executionInfo":{"status":"ok","timestamp":1617912528133,"user_tz":300,"elapsed":104626,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"8f0d8cb6-c973-4707-e65c-349f553cfb5c"},"source":["N_EPOCHS = 20\n","\n","best_valid_loss = float('inf')\n","\n","n_epochs_stop = 3\n","epochs_no_improve = 0\n","early_stop = False\n","\n","train_losses = []\n","valid_losses = []\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss, train_acc = train(model, training_loader, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, validating_loader, criterion)\n","    \n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","\n","    end_time = time.time()\n","        \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","        \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut6-model.pt')\n","    else:\n","        epochs_no_improve += 1\n","    \n","    # Early Stopping\n","    if epoch > 5 and epochs_no_improve == n_epochs_stop:\n","        print('Early stopping!' )\n","        early_stop = True\n","        break\n","    \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 7s\n","\tTrain Loss: 1.622 | Train Acc: 35.11%\n","\t Val. Loss: 1.434 |  Val. Acc: 37.78%\n","Epoch: 02 | Epoch Time: 0m 6s\n","\tTrain Loss: 1.316 | Train Acc: 49.91%\n","\t Val. Loss: 1.105 |  Val. Acc: 53.70%\n","Epoch: 03 | Epoch Time: 0m 7s\n","\tTrain Loss: 1.062 | Train Acc: 61.89%\n","\t Val. Loss: 0.865 |  Val. Acc: 64.07%\n","Epoch: 04 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.876 | Train Acc: 69.10%\n","\t Val. Loss: 0.690 |  Val. Acc: 74.07%\n","Epoch: 05 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.776 | Train Acc: 73.50%\n","\t Val. Loss: 0.601 |  Val. Acc: 79.26%\n","Epoch: 06 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.690 | Train Acc: 76.12%\n","\t Val. Loss: 0.520 |  Val. Acc: 82.22%\n","Epoch: 07 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.596 | Train Acc: 80.81%\n","\t Val. Loss: 0.522 |  Val. Acc: 78.52%\n","Epoch: 08 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.486 | Train Acc: 85.67%\n","\t Val. Loss: 0.547 |  Val. Acc: 80.00%\n","Epoch: 09 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.471 | Train Acc: 84.36%\n","\t Val. Loss: 0.511 |  Val. Acc: 82.22%\n","Epoch: 10 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.486 | Train Acc: 84.83%\n","\t Val. Loss: 0.487 |  Val. Acc: 82.22%\n","Epoch: 11 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.424 | Train Acc: 86.80%\n","\t Val. Loss: 0.487 |  Val. Acc: 83.70%\n","Epoch: 12 | Epoch Time: 0m 7s\n","\tTrain Loss: 0.364 | Train Acc: 88.48%\n","\t Val. Loss: 0.428 |  Val. Acc: 83.33%\n","Early stopping!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"apyGTC7z0O_x"},"source":["## Validation Loss vs Training Loss"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500},"id":"rsMjoQLEyP3u","executionInfo":{"status":"ok","timestamp":1617912541881,"user_tz":300,"elapsed":862,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"a35bb879-7cd6-415a-ea8a-5106f175d24e"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","x = [i for i in range(len(train_losses))]\n","fig = plt.figure(figsize=(10, 8))\n","sns.lineplot(x=x, y=train_losses, label=\"Train Losses\")\n","sns.lineplot(x=x, y=valid_losses, label=\"Valid Losses\")\n","plt.legend(prop={'size': 20})"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7fdfc4b872d0>"]},"metadata":{"tags":[]},"execution_count":26},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x576 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"IkoI_5k30isS"},"source":["## Accuracy on Validation Set"]},{"cell_type":"code","metadata":{"id":"YhQ0_ZE6VqXz"},"source":["model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3k3hQlk0n49r","executionInfo":{"status":"ok","timestamp":1617951135975,"user_tz":300,"elapsed":959,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"b653ac81-6b23-4440-c081-c4250229b5e8"},"source":["import time\n","print(time.time())\n","\n","model.eval()\n","test_pred = torch.empty(0, dtype=torch.long, device=\"cuda\")\n","for i, data in enumerate(validating_loader):\n","  ids = data['ids'].to(device, dtype = torch.long)\n","  mask = data['mask'].to(device, dtype = torch.long)\n","  pred = model(ids, mask)\n","  test_pred =  torch.cat((test_pred,pred),dim =0)\n","pred_val, pred_idx = torch.max(test_pred, dim=1)\n","\n","print(time.time())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1617951135.0412557\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["1617951135.689721\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yHn7VWz4ozZl"},"source":["validate_dataset[\"Prediction\"] = pred_idx.cpu().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LDf-qqbQqHGa","executionInfo":{"status":"ok","timestamp":1617950404962,"user_tz":300,"elapsed":225,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"d8cb82da-e097-436b-e902-e14db26d7bc3"},"source":["print(\"Accuracy: \", sum(validate_dataset[\"cat\"] == validate_dataset[\"Prediction\"]) / len(validate_dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy:  0.868421052631579\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vKAgUt-w0o3-"},"source":["## Confusion Matrix"]},{"cell_type":"code","metadata":{"id":"3Jg7A4mAq7-f"},"source":["from sklearn.metrics import confusion_matrix\n","confusion = confusion_matrix(validate_dataset[\"cat\"], validate_dataset[\"Prediction\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"id":"WU1zYl8qqv0O","executionInfo":{"status":"ok","timestamp":1617950417722,"user_tz":300,"elapsed":615,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"6f8f8c5d-d435-40a2-f703-e92ba4c34b09"},"source":["plt.figure(figsize = (10,8))\n","sns.heatmap(confusion, annot=True, cmap=\"YlGnBu\");\n","plt.ylabel(\"True Label\"); \n","plt.xlabel(\"Predicted Label\");\n","\n","# Note the color scheme here is not optimal"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x576 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"OkrCpaK_DJtE"},"source":["# Generalize the model & Score Normalization"]},{"cell_type":"code","metadata":{"id":"ugIBrydeFDO6"},"source":["from transformers import DistilBertModel\n","\n","class DistillBERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(DistillBERTClass, self).__init__()\n","        self.l1 = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.8)\n","        self.dropout2 = torch.nn.Dropout(0.5)\n","        self.classifier = torch.nn.Linear(768, 6)\n","\n","    def forward(self, input_ids, attention_mask):\n","        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = output_1[0]\n","        pooler = hidden_state[:, 0]\n","\n","        pooler = self.dropout2(pooler)\n","        pooler = self.pre_classifier(pooler)\n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","\n","        output = self.classifier(pooler)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_agN7WtQDO0t","colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["775b41a07481493c92196f68c488b106","4156eb1ee72e4b049cd4002a39ae19e8","fd967ed2d9bd47359bff7b07b7c52fca","e2538205981b4ee4955c9393c7413d17","5648a19bb57c4c43a581753074c5e94a","438a8ca8c60b4889a064350f4f7b4380","023a937b4df34132ba786543722810e0","cfa609f12c98430aa57324315116a5f0","f60d1b0774fb4574af40f96489198c69","4d3d6ba7a7b34e5ba36927fe5096bf3d","5bb4aa0365694f9c891e07b76af8d73f","5d2bf2f2c71f4ab0ba73756225acd2b9","e468a8de4221487ea760afc0f5029dd3","4758bd89546e436694983727f3058bf8","730a501b60824df5823575ea3bc3c28b","ac603a6c43ac48d59d7b8b9b591c484d"]},"executionInfo":{"status":"ok","timestamp":1618086277433,"user_tz":300,"elapsed":22998,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"d3ffb4c0-46a9-40b2-ecc2-0427bb55ba9a"},"source":["model = DistillBERTClass()\n","model.load_state_dict(torch.load('/content/drive/My Drive/Rebuild my Professor/models/classification_model.pt', map_location='cuda'))\n","model = model.to(device)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"775b41a07481493c92196f68c488b106","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f60d1b0774fb4574af40f96489198c69","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVsU0uzUEfbm","executionInfo":{"status":"ok","timestamp":1617951701776,"user_tz":300,"elapsed":67887,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"180c778f-f4be-4c5c-dd3a-045074ee97c9"},"source":["import os\n","from tqdm import tqdm\n","import random\n","from nltk.tokenize import sent_tokenize\n","import nltk\n","import scipy\n","# nltk.download('punkt')\n","\n","input_path = \"/content/drive/My Drive/Rebuild my Professor/Ratings/\"\n","files = [file for file in os.listdir(input_path) if file.endswith(\"csv\")]\n","data = pd.DataFrame()\n","for file in tqdm(files):\n","  df = pd.read_csv(input_path + file, engine=\"python\", index_col = 0)\n","  df = df[df.tid.isin(random.sample(set(df.tid.unique()),int(len(df.tid.unique()) / 20)))]\n","  data = data.append(df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 97/97 [01:07<00:00,  1.43it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"m7DgLp4BFL-o"},"source":["tid_sample = data['tid'].value_counts()[(data['tid'].value_counts().values >= 30) & (data['tid'].value_counts().values < 1000)]\n","tid_sample = list(tid_sample.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiV4zAwmHHF2"},"source":["data = data[data.tid.isin(tid_sample)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7uFa1lV_HlK_","executionInfo":{"status":"ok","timestamp":1617952319883,"user_tz":300,"elapsed":575053,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"938c48a8-274e-47c9-aa84-25cc30790c83"},"source":["model.eval()\n","scores = []\n","for tid in tqdm(tid_sample):\n","  sentences = list(data.loc[data.tid == tid, \"rComments\"])\n","  sentences = [sent for review in sentences if type(review)==str for sent in sent_tokenize(review)]\n","  sentences = pd.DataFrame({\"sentence\": sentences})\n","  sentences[\"cat\"] = 0\n","  sentences_dict = Triage(sentences, tokenizer, MAX_LEN)\n","  sentences_loader = DataLoader(sentences_dict, **test_params)\n","  model.eval()\n","  test_pred = []\n","  for i, batch in enumerate(sentences_loader):\n","    ids = batch['ids'].to(device)\n","    mask = batch['mask'].to(device)\n","    pred = model(ids, mask)\n","    pred = pred.cpu().detach().numpy()\n","    test_pred.extend(pred)\n","  prof_scores = scipy.special.softmax(test_pred, axis=1).mean(axis=0)\n","  scores.append(prof_scores)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/1096 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  0%|          | 1/1096 [00:04<1:30:30,  4.96s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  0%|          | 2/1096 [00:07<1:19:35,  4.37s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  0%|          | 3/1096 [00:10<1:08:56,  3.78s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  0%|          | 4/1096 [00:12<1:02:04,  3.41s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  0%|          | 5/1096 [00:15<58:19,  3.21s/it]  /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 6/1096 [00:18<54:07,  2.98s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 7/1096 [00:20<49:50,  2.75s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 8/1096 [00:22<47:57,  2.64s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 9/1096 [00:24<45:33,  2.51s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 10/1096 [00:26<42:40,  2.36s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 11/1096 [00:28<40:52,  2.26s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 12/1096 [00:30<38:46,  2.15s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|          | 13/1096 [00:32<37:47,  2.09s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|▏         | 14/1096 [00:34<37:40,  2.09s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|▏         | 15/1096 [00:36<37:42,  2.09s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  1%|▏         | 16/1096 [00:38<36:48,  2.05s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 17/1096 [00:40<34:56,  1.94s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 18/1096 [00:42<33:47,  1.88s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 19/1096 [00:44<33:48,  1.88s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 20/1096 [00:45<32:04,  1.79s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 21/1096 [00:47<31:33,  1.76s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 22/1096 [00:49<31:43,  1.77s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 23/1096 [00:50<30:59,  1.73s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 24/1096 [00:52<31:17,  1.75s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 25/1096 [00:54<31:16,  1.75s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 26/1096 [00:56<30:10,  1.69s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  2%|▏         | 27/1096 [00:57<30:03,  1.69s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 28/1096 [00:59<29:59,  1.68s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 29/1096 [01:00<28:20,  1.59s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 30/1096 [01:02<27:59,  1.58s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 31/1096 [01:03<27:02,  1.52s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 32/1096 [01:05<26:55,  1.52s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 33/1096 [01:06<26:06,  1.47s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 34/1096 [01:08<26:08,  1.48s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 35/1096 [01:09<24:48,  1.40s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 36/1096 [01:10<25:02,  1.42s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 37/1096 [01:12<24:18,  1.38s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  3%|▎         | 38/1096 [01:13<24:29,  1.39s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▎         | 39/1096 [01:14<24:39,  1.40s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▎         | 40/1096 [01:16<24:49,  1.41s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▎         | 41/1096 [01:17<23:59,  1.36s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▍         | 42/1096 [01:19<24:35,  1.40s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▍         | 43/1096 [01:20<23:37,  1.35s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▍         | 44/1096 [01:21<23:11,  1.32s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▍         | 45/1096 [01:22<22:27,  1.28s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▍         | 46/1096 [01:24<22:44,  1.30s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▍         | 47/1096 [01:25<21:42,  1.24s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▍         | 48/1096 [01:26<21:00,  1.20s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  4%|▍         | 49/1096 [01:27<21:04,  1.21s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▍         | 50/1096 [01:28<21:08,  1.21s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▍         | 51/1096 [01:29<20:13,  1.16s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▍         | 52/1096 [01:30<19:33,  1.12s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▍         | 53/1096 [01:31<19:17,  1.11s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▍         | 54/1096 [01:33<19:37,  1.13s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▌         | 55/1096 [01:34<19:10,  1.11s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▌         | 56/1096 [01:35<18:24,  1.06s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▌         | 57/1096 [01:36<18:30,  1.07s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▌         | 58/1096 [01:37<19:10,  1.11s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▌         | 59/1096 [01:38<19:47,  1.15s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  5%|▌         | 60/1096 [01:39<17:54,  1.04s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▌         | 61/1096 [01:40<18:03,  1.05s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▌         | 62/1096 [01:41<16:58,  1.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▌         | 63/1096 [01:42<17:09,  1.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▌         | 64/1096 [01:43<18:06,  1.05s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▌         | 65/1096 [01:44<18:12,  1.06s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▌         | 66/1096 [01:45<18:26,  1.07s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▌         | 67/1096 [01:46<17:35,  1.03s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▌         | 68/1096 [01:47<18:33,  1.08s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▋         | 69/1096 [01:48<18:12,  1.06s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▋         | 70/1096 [01:49<18:45,  1.10s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  6%|▋         | 71/1096 [01:51<19:02,  1.11s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 72/1096 [01:51<17:22,  1.02s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 73/1096 [01:52<16:14,  1.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 74/1096 [01:53<17:10,  1.01s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 75/1096 [01:55<18:01,  1.06s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 76/1096 [01:56<17:38,  1.04s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 77/1096 [01:56<17:06,  1.01s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 78/1096 [01:57<16:36,  1.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 79/1096 [01:59<17:23,  1.03s/it]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 80/1096 [01:59<16:54,  1.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 81/1096 [02:00<15:35,  1.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  7%|▋         | 82/1096 [02:01<16:04,  1.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 83/1096 [02:02<16:32,  1.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 84/1096 [02:03<16:34,  1.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 85/1096 [02:04<16:03,  1.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 86/1096 [02:05<15:29,  1.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 87/1096 [02:06<15:30,  1.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 88/1096 [02:07<15:02,  1.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 89/1096 [02:08<14:54,  1.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 90/1096 [02:09<15:42,  1.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 91/1096 [02:10<16:11,  1.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 92/1096 [02:11<16:13,  1.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  8%|▊         | 93/1096 [02:12<16:17,  1.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▊         | 94/1096 [02:13<16:21,  1.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▊         | 95/1096 [02:14<15:48,  1.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▉         | 96/1096 [02:14<15:41,  1.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▉         | 97/1096 [02:15<15:17,  1.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▉         | 98/1096 [02:16<14:59,  1.11it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▉         | 99/1096 [02:17<15:52,  1.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▉         | 100/1096 [02:18<15:57,  1.04it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▉         | 101/1096 [02:19<15:04,  1.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▉         | 102/1096 [02:20<15:20,  1.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▉         | 103/1096 [02:21<15:27,  1.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","  9%|▉         | 104/1096 [02:22<15:18,  1.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|▉         | 105/1096 [02:23<15:00,  1.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|▉         | 106/1096 [02:24<14:37,  1.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|▉         | 107/1096 [02:25<15:11,  1.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|▉         | 108/1096 [02:26<15:15,  1.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|▉         | 109/1096 [02:26<14:55,  1.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|█         | 110/1096 [02:27<14:02,  1.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|█         | 111/1096 [02:28<14:15,  1.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|█         | 112/1096 [02:29<14:17,  1.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|█         | 113/1096 [02:30<14:03,  1.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|█         | 114/1096 [02:31<13:52,  1.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 10%|█         | 115/1096 [02:31<13:54,  1.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█         | 116/1096 [02:32<13:52,  1.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█         | 117/1096 [02:33<14:27,  1.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█         | 118/1096 [02:34<14:15,  1.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█         | 119/1096 [02:35<14:45,  1.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█         | 120/1096 [02:36<14:33,  1.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█         | 121/1096 [02:37<13:48,  1.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█         | 122/1096 [02:37<13:14,  1.23it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█         | 123/1096 [02:38<13:03,  1.24it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█▏        | 124/1096 [02:39<13:02,  1.24it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█▏        | 125/1096 [02:40<12:22,  1.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 11%|█▏        | 126/1096 [02:41<13:08,  1.23it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 127/1096 [02:41<13:25,  1.20it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 128/1096 [02:42<13:30,  1.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 129/1096 [02:43<13:40,  1.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 130/1096 [02:44<13:49,  1.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 131/1096 [02:45<13:47,  1.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 132/1096 [02:46<13:58,  1.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 133/1096 [02:46<12:13,  1.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 134/1096 [02:47<12:20,  1.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 135/1096 [02:48<12:07,  1.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▏        | 136/1096 [02:49<12:01,  1.33it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 12%|█▎        | 137/1096 [02:49<12:16,  1.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 138/1096 [02:50<12:29,  1.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 139/1096 [02:51<12:09,  1.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 140/1096 [02:52<12:28,  1.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 141/1096 [02:52<12:19,  1.29it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 142/1096 [02:53<12:14,  1.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 143/1096 [02:54<11:55,  1.33it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 144/1096 [02:55<11:37,  1.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 145/1096 [02:55<11:03,  1.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 146/1096 [02:56<11:46,  1.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 13%|█▎        | 147/1096 [02:57<11:49,  1.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▎        | 148/1096 [02:58<11:59,  1.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▎        | 149/1096 [02:58<11:24,  1.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▎        | 150/1096 [02:59<10:57,  1.44it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▍        | 151/1096 [03:00<11:07,  1.42it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▍        | 152/1096 [03:00<11:33,  1.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▍        | 153/1096 [03:01<11:35,  1.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▍        | 154/1096 [03:02<11:21,  1.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▍        | 155/1096 [03:03<11:43,  1.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▍        | 156/1096 [03:03<11:53,  1.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▍        | 157/1096 [03:04<11:44,  1.33it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 14%|█▍        | 158/1096 [03:05<12:19,  1.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▍        | 159/1096 [03:06<12:33,  1.24it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▍        | 160/1096 [03:07<12:21,  1.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▍        | 161/1096 [03:08<12:40,  1.23it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▍        | 162/1096 [03:08<11:59,  1.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▍        | 163/1096 [03:09<11:39,  1.33it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▍        | 164/1096 [03:10<12:14,  1.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▌        | 165/1096 [03:11<12:16,  1.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▌        | 166/1096 [03:11<12:15,  1.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▌        | 167/1096 [03:12<12:13,  1.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▌        | 168/1096 [03:13<11:05,  1.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 15%|█▌        | 169/1096 [03:13<10:37,  1.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▌        | 170/1096 [03:14<10:24,  1.48it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▌        | 171/1096 [03:15<10:17,  1.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▌        | 172/1096 [03:15<10:47,  1.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▌        | 173/1096 [03:16<11:15,  1.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▌        | 174/1096 [03:17<11:23,  1.35it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▌        | 175/1096 [03:18<11:35,  1.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▌        | 176/1096 [03:18<11:06,  1.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▌        | 177/1096 [03:19<11:27,  1.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▌        | 178/1096 [03:20<10:49,  1.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▋        | 179/1096 [03:21<10:44,  1.42it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 16%|█▋        | 180/1096 [03:21<10:45,  1.42it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 181/1096 [03:22<10:40,  1.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 182/1096 [03:23<10:11,  1.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 183/1096 [03:23<10:23,  1.46it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 184/1096 [03:24<10:36,  1.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 185/1096 [03:25<10:34,  1.44it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 186/1096 [03:25<10:53,  1.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 187/1096 [03:26<10:24,  1.46it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 188/1096 [03:27<10:24,  1.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 189/1096 [03:27<10:36,  1.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 190/1096 [03:28<09:19,  1.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 17%|█▋        | 191/1096 [03:28<09:15,  1.63it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 192/1096 [03:29<09:19,  1.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 193/1096 [03:30<08:53,  1.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 194/1096 [03:30<09:01,  1.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 195/1096 [03:31<08:48,  1.70it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 196/1096 [03:31<09:03,  1.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 197/1096 [03:32<09:23,  1.59it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 198/1096 [03:33<09:05,  1.65it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 199/1096 [03:33<08:58,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 200/1096 [03:34<08:52,  1.68it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 201/1096 [03:35<09:17,  1.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 18%|█▊        | 202/1096 [03:35<09:29,  1.57it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▊        | 203/1096 [03:36<09:24,  1.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▊        | 204/1096 [03:36<09:21,  1.59it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▊        | 205/1096 [03:37<09:54,  1.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▉        | 206/1096 [03:38<09:19,  1.59it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▉        | 207/1096 [03:38<09:01,  1.64it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▉        | 208/1096 [03:39<08:53,  1.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▉        | 209/1096 [03:40<09:23,  1.57it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▉        | 210/1096 [03:40<09:09,  1.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▉        | 211/1096 [03:41<09:04,  1.63it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▉        | 212/1096 [03:41<08:50,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 19%|█▉        | 213/1096 [03:42<08:30,  1.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|█▉        | 214/1096 [03:43<08:53,  1.65it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|█▉        | 215/1096 [03:43<08:22,  1.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|█▉        | 216/1096 [03:44<08:35,  1.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|█▉        | 217/1096 [03:44<08:32,  1.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|█▉        | 218/1096 [03:45<08:46,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|█▉        | 219/1096 [03:46<09:47,  1.49it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|██        | 220/1096 [03:47<10:11,  1.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|██        | 221/1096 [03:47<09:48,  1.49it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|██        | 222/1096 [03:48<09:08,  1.59it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|██        | 223/1096 [03:48<09:22,  1.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 20%|██        | 224/1096 [03:49<09:13,  1.57it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██        | 225/1096 [03:50<09:14,  1.57it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██        | 226/1096 [03:50<09:04,  1.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██        | 227/1096 [03:51<08:58,  1.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██        | 228/1096 [03:51<09:10,  1.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██        | 229/1096 [03:52<09:01,  1.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██        | 230/1096 [03:53<08:49,  1.64it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██        | 231/1096 [03:53<08:56,  1.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██        | 232/1096 [03:54<08:29,  1.70it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██▏       | 233/1096 [03:54<08:36,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██▏       | 234/1096 [03:55<08:04,  1.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 21%|██▏       | 235/1096 [03:55<08:08,  1.76it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 236/1096 [03:56<08:09,  1.76it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 237/1096 [03:57<07:59,  1.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 238/1096 [03:57<08:11,  1.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 239/1096 [03:58<08:32,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 240/1096 [03:58<08:45,  1.63it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 241/1096 [03:59<08:50,  1.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 242/1096 [04:00<09:10,  1.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 243/1096 [04:00<09:11,  1.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 244/1096 [04:01<08:58,  1.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 245/1096 [04:02<08:50,  1.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 22%|██▏       | 246/1096 [04:02<09:06,  1.56it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 247/1096 [04:03<08:49,  1.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 248/1096 [04:03<08:16,  1.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 249/1096 [04:04<08:25,  1.68it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 250/1096 [04:05<08:23,  1.68it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 251/1096 [04:05<07:53,  1.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 252/1096 [04:06<08:01,  1.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 253/1096 [04:06<08:20,  1.68it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 254/1096 [04:07<08:38,  1.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 255/1096 [04:08<08:47,  1.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 256/1096 [04:08<08:34,  1.63it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 23%|██▎       | 257/1096 [04:09<08:29,  1.65it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▎       | 258/1096 [04:09<08:14,  1.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▎       | 259/1096 [04:10<08:03,  1.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▎       | 260/1096 [04:11<07:49,  1.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▍       | 261/1096 [04:11<08:08,  1.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▍       | 262/1096 [04:12<07:49,  1.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▍       | 263/1096 [04:12<07:46,  1.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▍       | 264/1096 [04:13<07:44,  1.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▍       | 265/1096 [04:13<08:19,  1.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▍       | 266/1096 [04:14<08:05,  1.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▍       | 267/1096 [04:15<08:11,  1.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 24%|██▍       | 268/1096 [04:15<08:19,  1.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▍       | 269/1096 [04:16<07:43,  1.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▍       | 270/1096 [04:16<08:00,  1.72it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▍       | 271/1096 [04:17<07:52,  1.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▍       | 272/1096 [04:17<07:51,  1.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▍       | 273/1096 [04:18<08:12,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▌       | 274/1096 [04:19<08:07,  1.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▌       | 275/1096 [04:19<08:15,  1.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▌       | 276/1096 [04:20<07:49,  1.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▌       | 277/1096 [04:20<08:01,  1.70it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▌       | 278/1096 [04:21<08:10,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 25%|██▌       | 279/1096 [04:22<08:30,  1.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▌       | 280/1096 [04:22<08:37,  1.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▌       | 281/1096 [04:23<08:03,  1.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▌       | 282/1096 [04:23<07:51,  1.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▌       | 283/1096 [04:24<08:07,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▌       | 284/1096 [04:25<07:53,  1.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▌       | 285/1096 [04:25<07:58,  1.70it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▌       | 286/1096 [04:26<07:16,  1.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▌       | 287/1096 [04:26<07:37,  1.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▋       | 288/1096 [04:27<07:46,  1.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▋       | 289/1096 [04:27<07:48,  1.72it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 26%|██▋       | 290/1096 [04:28<07:49,  1.72it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 291/1096 [04:29<07:43,  1.74it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 292/1096 [04:29<07:45,  1.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 293/1096 [04:30<07:31,  1.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 294/1096 [04:30<07:45,  1.72it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 295/1096 [04:31<08:04,  1.65it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 296/1096 [04:32<08:02,  1.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 297/1096 [04:32<08:13,  1.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 298/1096 [04:33<08:33,  1.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 299/1096 [04:33<07:57,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 300/1096 [04:34<08:04,  1.64it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 27%|██▋       | 301/1096 [04:35<07:56,  1.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 302/1096 [04:35<07:31,  1.76it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 303/1096 [04:36<07:28,  1.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 304/1096 [04:36<07:36,  1.74it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 305/1096 [04:37<07:21,  1.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 306/1096 [04:37<07:08,  1.84it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 307/1096 [04:38<07:00,  1.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 308/1096 [04:38<07:12,  1.82it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 309/1096 [04:39<07:03,  1.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 310/1096 [04:40<07:21,  1.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 311/1096 [04:40<07:11,  1.82it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 28%|██▊       | 312/1096 [04:41<07:20,  1.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▊       | 313/1096 [04:41<07:27,  1.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▊       | 314/1096 [04:42<07:25,  1.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▊       | 315/1096 [04:42<07:33,  1.72it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▉       | 316/1096 [04:43<07:20,  1.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▉       | 317/1096 [04:44<07:09,  1.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▉       | 318/1096 [04:44<07:17,  1.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▉       | 319/1096 [04:45<06:52,  1.89it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▉       | 320/1096 [04:45<06:38,  1.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▉       | 321/1096 [04:46<06:47,  1.90it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▉       | 322/1096 [04:46<07:05,  1.82it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 29%|██▉       | 323/1096 [04:47<06:36,  1.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|██▉       | 324/1096 [04:47<06:36,  1.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|██▉       | 325/1096 [04:48<06:37,  1.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|██▉       | 326/1096 [04:48<06:46,  1.89it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|██▉       | 327/1096 [04:49<06:34,  1.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|██▉       | 328/1096 [04:49<06:33,  1.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|███       | 329/1096 [04:50<06:56,  1.84it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|███       | 330/1096 [04:50<06:33,  1.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|███       | 331/1096 [04:51<06:18,  2.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|███       | 332/1096 [04:51<06:33,  1.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|███       | 333/1096 [04:52<06:27,  1.97it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 30%|███       | 334/1096 [04:52<06:18,  2.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███       | 335/1096 [04:53<06:20,  2.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███       | 336/1096 [04:53<06:28,  1.96it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███       | 337/1096 [04:54<06:19,  2.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███       | 338/1096 [04:54<06:30,  1.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███       | 339/1096 [04:55<06:42,  1.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███       | 340/1096 [04:55<06:38,  1.90it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███       | 341/1096 [04:56<06:23,  1.97it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███       | 342/1096 [04:56<06:36,  1.90it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███▏      | 343/1096 [04:57<06:49,  1.84it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███▏      | 344/1096 [04:57<06:11,  2.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 31%|███▏      | 345/1096 [04:58<06:13,  2.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 346/1096 [04:58<06:31,  1.92it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 347/1096 [04:59<06:26,  1.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 348/1096 [05:00<06:34,  1.89it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 349/1096 [05:00<06:23,  1.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 350/1096 [05:00<05:49,  2.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 351/1096 [05:01<06:00,  2.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 352/1096 [05:01<06:23,  1.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 353/1096 [05:02<06:05,  2.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 354/1096 [05:02<06:06,  2.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 355/1096 [05:03<06:11,  2.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 32%|███▏      | 356/1096 [05:03<06:21,  1.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 357/1096 [05:04<05:54,  2.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 358/1096 [05:04<05:57,  2.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 359/1096 [05:05<06:06,  2.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 360/1096 [05:05<05:53,  2.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 361/1096 [05:06<05:00,  2.44it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 362/1096 [05:06<05:33,  2.20it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 363/1096 [05:07<05:48,  2.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 364/1096 [05:07<06:00,  2.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 365/1096 [05:08<06:20,  1.92it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 366/1096 [05:08<06:12,  1.96it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 33%|███▎      | 367/1096 [05:09<05:59,  2.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▎      | 368/1096 [05:09<05:53,  2.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▎      | 369/1096 [05:10<06:02,  2.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▍      | 370/1096 [05:10<05:31,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▍      | 371/1096 [05:11<05:52,  2.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▍      | 372/1096 [05:11<05:30,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▍      | 373/1096 [05:12<05:36,  2.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▍      | 374/1096 [05:12<05:37,  2.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▍      | 375/1096 [05:12<05:29,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▍      | 376/1096 [05:13<05:28,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▍      | 377/1096 [05:13<05:46,  2.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 34%|███▍      | 378/1096 [05:14<05:51,  2.04it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▍      | 379/1096 [05:15<06:11,  1.93it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▍      | 380/1096 [05:15<05:53,  2.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▍      | 381/1096 [05:15<05:59,  1.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▍      | 382/1096 [05:16<06:14,  1.91it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▍      | 383/1096 [05:17<06:14,  1.90it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▌      | 384/1096 [05:17<06:19,  1.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▌      | 385/1096 [05:18<06:21,  1.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▌      | 386/1096 [05:18<06:19,  1.87it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▌      | 387/1096 [05:19<06:16,  1.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▌      | 388/1096 [05:19<06:16,  1.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 35%|███▌      | 389/1096 [05:20<06:06,  1.93it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▌      | 390/1096 [05:20<05:51,  2.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▌      | 391/1096 [05:21<05:49,  2.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▌      | 392/1096 [05:21<05:47,  2.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▌      | 393/1096 [05:22<05:49,  2.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▌      | 394/1096 [05:22<05:29,  2.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▌      | 395/1096 [05:23<05:43,  2.04it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▌      | 396/1096 [05:23<05:44,  2.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▌      | 397/1096 [05:24<05:22,  2.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▋      | 398/1096 [05:24<05:15,  2.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▋      | 399/1096 [05:24<05:23,  2.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 36%|███▋      | 400/1096 [05:25<05:39,  2.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 401/1096 [05:25<05:16,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 402/1096 [05:26<05:43,  2.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 403/1096 [05:26<05:42,  2.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 404/1096 [05:27<05:29,  2.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 405/1096 [05:27<05:36,  2.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 406/1096 [05:28<05:21,  2.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 407/1096 [05:28<05:26,  2.11it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 408/1096 [05:29<05:16,  2.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 409/1096 [05:29<05:14,  2.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 37%|███▋      | 410/1096 [05:30<05:17,  2.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 411/1096 [05:30<05:26,  2.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 412/1096 [05:31<05:22,  2.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 413/1096 [05:31<05:24,  2.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 414/1096 [05:32<05:22,  2.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 415/1096 [05:32<05:30,  2.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 416/1096 [05:33<05:27,  2.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 417/1096 [05:33<05:15,  2.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 418/1096 [05:33<05:10,  2.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 419/1096 [05:34<05:24,  2.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 420/1096 [05:34<05:21,  2.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 38%|███▊      | 421/1096 [05:35<05:24,  2.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▊      | 422/1096 [05:35<05:08,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▊      | 423/1096 [05:36<05:26,  2.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▊      | 424/1096 [05:36<05:17,  2.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▉      | 425/1096 [05:37<05:16,  2.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▉      | 426/1096 [05:37<05:21,  2.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▉      | 427/1096 [05:38<05:28,  2.04it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▉      | 428/1096 [05:38<05:38,  1.98it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▉      | 429/1096 [05:39<05:57,  1.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▉      | 430/1096 [05:39<05:54,  1.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▉      | 431/1096 [05:40<05:43,  1.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 39%|███▉      | 432/1096 [05:40<05:49,  1.90it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|███▉      | 433/1096 [05:41<05:26,  2.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|███▉      | 434/1096 [05:41<05:20,  2.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|███▉      | 435/1096 [05:42<05:02,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|███▉      | 436/1096 [05:42<05:14,  2.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|███▉      | 437/1096 [05:43<05:13,  2.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|███▉      | 438/1096 [05:43<05:01,  2.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|████      | 439/1096 [05:44<05:02,  2.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|████      | 440/1096 [05:44<05:08,  2.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|████      | 441/1096 [05:45<04:57,  2.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|████      | 442/1096 [05:45<05:01,  2.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 40%|████      | 443/1096 [05:45<04:57,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████      | 444/1096 [05:46<04:57,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████      | 445/1096 [05:46<04:47,  2.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████      | 446/1096 [05:47<04:49,  2.25it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████      | 447/1096 [05:47<05:07,  2.11it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████      | 448/1096 [05:48<04:24,  2.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████      | 449/1096 [05:48<04:26,  2.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████      | 450/1096 [05:48<04:13,  2.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████      | 451/1096 [05:49<04:13,  2.54it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████      | 452/1096 [05:49<04:27,  2.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████▏     | 453/1096 [05:50<04:51,  2.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 41%|████▏     | 454/1096 [05:50<04:52,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 455/1096 [05:51<04:58,  2.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 456/1096 [05:51<05:06,  2.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 457/1096 [05:52<05:08,  2.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 458/1096 [05:52<04:52,  2.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 459/1096 [05:53<04:59,  2.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 460/1096 [05:53<04:57,  2.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 461/1096 [05:53<04:36,  2.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 462/1096 [05:54<04:34,  2.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 463/1096 [05:54<04:44,  2.23it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 464/1096 [05:55<04:54,  2.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 42%|████▏     | 465/1096 [05:55<04:52,  2.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 466/1096 [05:56<04:54,  2.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 467/1096 [05:56<04:51,  2.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 468/1096 [05:57<04:58,  2.11it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 469/1096 [05:57<05:03,  2.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 470/1096 [05:58<05:02,  2.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 471/1096 [05:58<04:58,  2.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 472/1096 [05:59<04:58,  2.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 473/1096 [05:59<04:36,  2.25it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 474/1096 [05:59<04:25,  2.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 475/1096 [06:00<04:31,  2.29it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 43%|████▎     | 476/1096 [06:00<04:18,  2.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▎     | 477/1096 [06:01<04:17,  2.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▎     | 478/1096 [06:01<04:33,  2.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▎     | 479/1096 [06:02<04:34,  2.25it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▍     | 480/1096 [06:02<04:25,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▍     | 481/1096 [06:02<04:27,  2.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▍     | 482/1096 [06:03<04:27,  2.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▍     | 483/1096 [06:03<04:37,  2.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▍     | 484/1096 [06:04<04:25,  2.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▍     | 485/1096 [06:04<04:43,  2.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▍     | 486/1096 [06:05<04:40,  2.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 44%|████▍     | 487/1096 [06:05<04:43,  2.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▍     | 488/1096 [06:06<04:43,  2.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▍     | 489/1096 [06:06<04:39,  2.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▍     | 490/1096 [06:07<04:47,  2.11it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▍     | 491/1096 [06:07<04:40,  2.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▍     | 492/1096 [06:08<04:30,  2.24it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▍     | 493/1096 [06:08<04:34,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▌     | 494/1096 [06:08<04:37,  2.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▌     | 495/1096 [06:09<04:31,  2.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▌     | 496/1096 [06:09<04:25,  2.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▌     | 497/1096 [06:10<04:13,  2.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 45%|████▌     | 498/1096 [06:10<04:22,  2.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▌     | 499/1096 [06:11<04:25,  2.25it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▌     | 500/1096 [06:11<04:12,  2.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▌     | 501/1096 [06:11<04:22,  2.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▌     | 502/1096 [06:12<04:20,  2.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▌     | 503/1096 [06:12<04:31,  2.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▌     | 504/1096 [06:13<03:58,  2.48it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▌     | 505/1096 [06:13<03:53,  2.53it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▌     | 506/1096 [06:14<04:06,  2.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▋     | 507/1096 [06:14<04:06,  2.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▋     | 508/1096 [06:14<04:18,  2.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 46%|████▋     | 509/1096 [06:15<04:13,  2.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 510/1096 [06:15<04:13,  2.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 511/1096 [06:16<04:05,  2.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 512/1096 [06:16<04:11,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 513/1096 [06:17<04:04,  2.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 514/1096 [06:17<04:08,  2.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 515/1096 [06:17<04:10,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 516/1096 [06:18<04:06,  2.35it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 517/1096 [06:18<04:10,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 518/1096 [06:19<04:21,  2.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 519/1096 [06:19<04:13,  2.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 47%|████▋     | 520/1096 [06:19<03:34,  2.68it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 521/1096 [06:20<03:52,  2.47it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 522/1096 [06:20<04:07,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 523/1096 [06:21<04:06,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 524/1096 [06:21<04:11,  2.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 525/1096 [06:22<04:07,  2.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 526/1096 [06:22<04:00,  2.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 527/1096 [06:23<04:05,  2.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 528/1096 [06:23<04:03,  2.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 529/1096 [06:23<04:01,  2.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 530/1096 [06:24<03:57,  2.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 48%|████▊     | 531/1096 [06:24<03:47,  2.48it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▊     | 532/1096 [06:25<03:37,  2.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▊     | 533/1096 [06:25<03:30,  2.68it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▊     | 534/1096 [06:25<03:15,  2.87it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▉     | 535/1096 [06:26<03:33,  2.63it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▉     | 536/1096 [06:26<03:43,  2.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▉     | 537/1096 [06:26<03:53,  2.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▉     | 538/1096 [06:27<03:49,  2.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▉     | 539/1096 [06:27<03:34,  2.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▉     | 540/1096 [06:28<03:37,  2.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▉     | 541/1096 [06:28<04:01,  2.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 49%|████▉     | 542/1096 [06:29<03:58,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|████▉     | 543/1096 [06:29<04:00,  2.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|████▉     | 544/1096 [06:30<04:11,  2.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|████▉     | 545/1096 [06:30<04:15,  2.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|████▉     | 546/1096 [06:31<04:19,  2.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|████▉     | 547/1096 [06:31<04:18,  2.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|█████     | 548/1096 [06:31<04:07,  2.22it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|█████     | 549/1096 [06:32<03:56,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|█████     | 550/1096 [06:32<03:50,  2.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|█████     | 551/1096 [06:32<03:26,  2.64it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|█████     | 552/1096 [06:33<03:26,  2.63it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 50%|█████     | 553/1096 [06:33<03:33,  2.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████     | 554/1096 [06:34<03:39,  2.47it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████     | 555/1096 [06:34<03:42,  2.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████     | 556/1096 [06:35<03:44,  2.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████     | 557/1096 [06:35<03:43,  2.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████     | 558/1096 [06:35<03:50,  2.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████     | 559/1096 [06:36<03:43,  2.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████     | 560/1096 [06:36<03:41,  2.42it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████     | 561/1096 [06:37<03:40,  2.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████▏    | 562/1096 [06:37<03:38,  2.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████▏    | 563/1096 [06:37<03:26,  2.59it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 51%|█████▏    | 564/1096 [06:38<03:29,  2.53it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 565/1096 [06:38<03:31,  2.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 566/1096 [06:39<03:30,  2.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 567/1096 [06:39<03:35,  2.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 568/1096 [06:39<03:35,  2.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 569/1096 [06:40<03:32,  2.48it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 570/1096 [06:40<03:31,  2.49it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 571/1096 [06:41<03:29,  2.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 572/1096 [06:41<03:08,  2.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 573/1096 [06:41<03:13,  2.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 574/1096 [06:42<03:32,  2.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 52%|█████▏    | 575/1096 [06:42<03:31,  2.46it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 576/1096 [06:43<03:28,  2.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 577/1096 [06:43<03:22,  2.56it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 578/1096 [06:43<03:19,  2.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 579/1096 [06:44<03:37,  2.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 580/1096 [06:44<03:37,  2.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 581/1096 [06:45<03:34,  2.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 582/1096 [06:45<03:30,  2.44it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 583/1096 [06:45<03:27,  2.47it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 584/1096 [06:46<03:35,  2.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 585/1096 [06:46<03:24,  2.49it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 53%|█████▎    | 586/1096 [06:47<03:24,  2.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▎    | 587/1096 [06:47<03:11,  2.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▎    | 588/1096 [06:47<03:24,  2.48it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▎    | 589/1096 [06:48<03:24,  2.48it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▍    | 590/1096 [06:48<03:19,  2.53it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▍    | 591/1096 [06:49<03:18,  2.54it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▍    | 592/1096 [06:49<03:04,  2.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▍    | 593/1096 [06:49<02:58,  2.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▍    | 594/1096 [06:50<03:12,  2.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▍    | 595/1096 [06:50<03:19,  2.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▍    | 596/1096 [06:51<03:25,  2.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 54%|█████▍    | 597/1096 [06:51<03:38,  2.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▍    | 598/1096 [06:51<03:26,  2.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▍    | 599/1096 [06:52<03:29,  2.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▍    | 600/1096 [06:52<03:24,  2.42it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▍    | 601/1096 [06:53<03:31,  2.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▍    | 602/1096 [06:53<03:28,  2.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▌    | 603/1096 [06:54<03:38,  2.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▌    | 604/1096 [06:54<03:32,  2.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▌    | 605/1096 [06:54<03:31,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▌    | 606/1096 [06:55<03:30,  2.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▌    | 607/1096 [06:55<03:20,  2.44it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 55%|█████▌    | 608/1096 [06:56<03:19,  2.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▌    | 609/1096 [06:56<03:23,  2.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▌    | 610/1096 [06:56<03:16,  2.47it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▌    | 611/1096 [06:57<03:03,  2.64it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▌    | 612/1096 [06:57<03:08,  2.57it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▌    | 613/1096 [06:58<03:19,  2.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▌    | 614/1096 [06:58<03:12,  2.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▌    | 615/1096 [06:58<03:01,  2.65it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▌    | 616/1096 [06:59<03:02,  2.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▋    | 617/1096 [06:59<03:15,  2.46it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▋    | 618/1096 [07:00<03:14,  2.46it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 56%|█████▋    | 619/1096 [07:00<03:10,  2.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 620/1096 [07:00<03:13,  2.46it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 621/1096 [07:01<03:14,  2.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 622/1096 [07:01<03:06,  2.54it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 623/1096 [07:02<03:02,  2.59it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 624/1096 [07:02<02:54,  2.70it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 625/1096 [07:02<02:59,  2.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 626/1096 [07:03<02:59,  2.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 627/1096 [07:03<02:59,  2.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 628/1096 [07:03<02:56,  2.65it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 629/1096 [07:04<02:50,  2.74it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 57%|█████▋    | 630/1096 [07:04<02:50,  2.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 631/1096 [07:04<02:30,  3.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 632/1096 [07:05<02:43,  2.84it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 633/1096 [07:05<02:34,  2.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 634/1096 [07:05<02:43,  2.83it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 635/1096 [07:06<02:48,  2.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 636/1096 [07:06<02:51,  2.68it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 637/1096 [07:07<02:44,  2.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 638/1096 [07:07<02:47,  2.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 639/1096 [07:07<02:46,  2.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 640/1096 [07:08<02:41,  2.82it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 58%|█████▊    | 641/1096 [07:08<02:35,  2.93it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▊    | 642/1096 [07:08<02:39,  2.85it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▊    | 643/1096 [07:09<02:42,  2.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▉    | 644/1096 [07:09<02:50,  2.65it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▉    | 645/1096 [07:10<02:53,  2.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▉    | 646/1096 [07:10<02:59,  2.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▉    | 647/1096 [07:10<03:07,  2.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▉    | 648/1096 [07:11<02:55,  2.56it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▉    | 649/1096 [07:11<02:47,  2.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▉    | 650/1096 [07:11<02:43,  2.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▉    | 651/1096 [07:12<02:41,  2.76it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 59%|█████▉    | 652/1096 [07:12<02:43,  2.72it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|█████▉    | 653/1096 [07:12<02:40,  2.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|█████▉    | 654/1096 [07:13<02:38,  2.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|█████▉    | 655/1096 [07:13<02:44,  2.68it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|█████▉    | 656/1096 [07:14<02:48,  2.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|█████▉    | 657/1096 [07:14<02:49,  2.59it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|██████    | 658/1096 [07:14<02:49,  2.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|██████    | 659/1096 [07:15<02:50,  2.57it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|██████    | 660/1096 [07:15<02:52,  2.53it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|██████    | 661/1096 [07:16<02:53,  2.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|██████    | 662/1096 [07:16<02:52,  2.52it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 60%|██████    | 663/1096 [07:16<02:39,  2.72it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████    | 664/1096 [07:17<02:35,  2.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████    | 665/1096 [07:17<02:37,  2.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████    | 666/1096 [07:17<02:39,  2.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████    | 667/1096 [07:18<02:47,  2.56it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████    | 668/1096 [07:18<02:43,  2.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████    | 669/1096 [07:19<02:45,  2.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████    | 670/1096 [07:19<02:47,  2.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████    | 671/1096 [07:19<02:46,  2.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████▏   | 672/1096 [07:20<02:49,  2.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████▏   | 673/1096 [07:20<02:31,  2.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 61%|██████▏   | 674/1096 [07:20<02:30,  2.80it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 675/1096 [07:21<02:29,  2.82it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 676/1096 [07:21<02:23,  2.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 677/1096 [07:22<02:34,  2.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 678/1096 [07:22<02:36,  2.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 679/1096 [07:22<02:11,  3.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 680/1096 [07:23<02:25,  2.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 681/1096 [07:23<02:13,  3.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 682/1096 [07:23<02:18,  3.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 683/1096 [07:24<02:24,  2.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▏   | 684/1096 [07:24<02:25,  2.84it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 62%|██████▎   | 685/1096 [07:24<02:34,  2.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 686/1096 [07:25<02:40,  2.56it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 687/1096 [07:25<02:43,  2.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 688/1096 [07:25<02:27,  2.76it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 689/1096 [07:26<02:28,  2.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 690/1096 [07:26<02:24,  2.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 691/1096 [07:27<02:29,  2.70it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 692/1096 [07:27<02:26,  2.76it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 693/1096 [07:27<02:25,  2.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 694/1096 [07:28<02:39,  2.52it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 63%|██████▎   | 695/1096 [07:28<02:32,  2.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▎   | 696/1096 [07:28<02:25,  2.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▎   | 697/1096 [07:29<02:27,  2.70it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▎   | 698/1096 [07:29<02:26,  2.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▍   | 699/1096 [07:30<02:23,  2.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▍   | 700/1096 [07:30<02:20,  2.82it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▍   | 701/1096 [07:30<02:15,  2.91it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▍   | 702/1096 [07:31<02:22,  2.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▍   | 703/1096 [07:31<02:20,  2.80it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▍   | 704/1096 [07:31<02:23,  2.74it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▍   | 705/1096 [07:32<02:28,  2.64it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 64%|██████▍   | 706/1096 [07:32<02:16,  2.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▍   | 707/1096 [07:32<02:10,  2.98it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▍   | 708/1096 [07:33<02:02,  3.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▍   | 709/1096 [07:33<02:11,  2.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▍   | 710/1096 [07:33<02:15,  2.85it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▍   | 711/1096 [07:34<02:13,  2.89it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▍   | 712/1096 [07:34<02:06,  3.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▌   | 713/1096 [07:34<02:13,  2.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▌   | 714/1096 [07:35<02:07,  2.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▌   | 715/1096 [07:35<02:02,  3.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▌   | 716/1096 [07:35<02:14,  2.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 65%|██████▌   | 717/1096 [07:36<02:14,  2.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▌   | 718/1096 [07:36<02:10,  2.90it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▌   | 719/1096 [07:36<02:11,  2.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▌   | 720/1096 [07:37<02:12,  2.83it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▌   | 721/1096 [07:37<02:06,  2.97it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▌   | 722/1096 [07:37<02:11,  2.84it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▌   | 723/1096 [07:38<02:10,  2.85it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▌   | 724/1096 [07:38<02:14,  2.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▌   | 725/1096 [07:39<02:16,  2.72it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▌   | 726/1096 [07:39<02:11,  2.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▋   | 727/1096 [07:39<02:06,  2.93it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 66%|██████▋   | 728/1096 [07:40<02:07,  2.89it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 729/1096 [07:40<02:08,  2.85it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 730/1096 [07:40<01:58,  3.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 731/1096 [07:41<02:04,  2.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 732/1096 [07:41<02:06,  2.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 733/1096 [07:41<02:10,  2.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 734/1096 [07:42<01:59,  3.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 735/1096 [07:42<02:01,  2.98it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 736/1096 [07:42<01:56,  3.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 737/1096 [07:43<01:59,  3.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 738/1096 [07:43<01:55,  3.11it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 67%|██████▋   | 739/1096 [07:43<02:03,  2.89it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 740/1096 [07:44<02:06,  2.80it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 741/1096 [07:44<01:46,  3.33it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 742/1096 [07:44<01:45,  3.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 743/1096 [07:44<01:42,  3.46it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 744/1096 [07:45<01:42,  3.44it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 745/1096 [07:45<01:51,  3.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 746/1096 [07:45<01:56,  3.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 747/1096 [07:46<01:56,  3.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 748/1096 [07:46<01:59,  2.91it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 749/1096 [07:46<01:49,  3.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 68%|██████▊   | 750/1096 [07:47<01:55,  3.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▊   | 751/1096 [07:47<01:56,  2.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▊   | 752/1096 [07:48<02:00,  2.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▊   | 753/1096 [07:48<01:54,  3.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▉   | 754/1096 [07:48<01:52,  3.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▉   | 755/1096 [07:48<01:50,  3.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▉   | 756/1096 [07:49<01:44,  3.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▉   | 757/1096 [07:49<01:50,  3.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▉   | 758/1096 [07:49<01:37,  3.48it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▉   | 759/1096 [07:50<01:43,  3.24it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▉   | 760/1096 [07:50<01:47,  3.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 69%|██████▉   | 761/1096 [07:50<01:44,  3.22it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|██████▉   | 762/1096 [07:51<01:44,  3.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|██████▉   | 763/1096 [07:51<01:55,  2.89it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|██████▉   | 764/1096 [07:51<01:58,  2.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|██████▉   | 765/1096 [07:52<02:02,  2.70it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|██████▉   | 766/1096 [07:52<01:59,  2.77it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|██████▉   | 767/1096 [07:53<01:59,  2.74it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|███████   | 768/1096 [07:53<01:55,  2.83it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|███████   | 769/1096 [07:53<01:51,  2.92it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|███████   | 770/1096 [07:53<01:47,  3.04it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|███████   | 771/1096 [07:54<01:46,  3.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 70%|███████   | 772/1096 [07:54<01:44,  3.11it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████   | 773/1096 [07:54<01:46,  3.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████   | 774/1096 [07:55<01:46,  3.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████   | 775/1096 [07:55<01:54,  2.80it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████   | 776/1096 [07:56<01:54,  2.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████   | 777/1096 [07:56<01:50,  2.89it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████   | 778/1096 [07:56<01:33,  3.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████   | 779/1096 [07:56<01:37,  3.24it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████   | 780/1096 [07:57<01:46,  2.97it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████▏  | 781/1096 [07:57<01:44,  3.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████▏  | 782/1096 [07:57<01:45,  2.98it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 71%|███████▏  | 783/1096 [07:58<01:43,  3.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 784/1096 [07:58<01:45,  2.96it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 785/1096 [07:58<01:46,  2.91it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 786/1096 [07:59<01:47,  2.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 787/1096 [07:59<01:43,  3.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 788/1096 [07:59<01:45,  2.91it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 789/1096 [08:00<01:45,  2.91it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 790/1096 [08:00<01:42,  2.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 791/1096 [08:01<01:46,  2.87it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 792/1096 [08:01<01:47,  2.83it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 793/1096 [08:01<01:46,  2.83it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 72%|███████▏  | 794/1096 [08:02<01:41,  2.98it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 795/1096 [08:02<01:35,  3.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 796/1096 [08:02<01:41,  2.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 797/1096 [08:03<01:46,  2.80it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 798/1096 [08:03<01:48,  2.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 799/1096 [08:03<01:44,  2.85it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 800/1096 [08:04<01:45,  2.80it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 801/1096 [08:04<01:45,  2.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 802/1096 [08:04<01:45,  2.80it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 803/1096 [08:05<01:42,  2.87it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 804/1096 [08:05<01:46,  2.73it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 73%|███████▎  | 805/1096 [08:05<01:43,  2.82it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▎  | 806/1096 [08:06<01:42,  2.84it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▎  | 807/1096 [08:06<01:43,  2.80it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▎  | 808/1096 [08:07<01:43,  2.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▍  | 809/1096 [08:07<01:44,  2.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▍  | 810/1096 [08:07<01:39,  2.86it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▍  | 811/1096 [08:08<01:39,  2.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▍  | 812/1096 [08:08<01:37,  2.92it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▍  | 813/1096 [08:08<01:33,  3.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▍  | 814/1096 [08:09<01:35,  2.96it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▍  | 815/1096 [08:09<01:31,  3.08it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 74%|███████▍  | 816/1096 [08:09<01:28,  3.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▍  | 817/1096 [08:09<01:29,  3.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▍  | 818/1096 [08:10<01:27,  3.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▍  | 819/1096 [08:10<01:19,  3.48it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▍  | 820/1096 [08:10<01:22,  3.35it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▍  | 821/1096 [08:11<01:21,  3.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▌  | 822/1096 [08:11<01:24,  3.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▌  | 823/1096 [08:11<01:25,  3.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▌  | 824/1096 [08:12<01:28,  3.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▌  | 825/1096 [08:12<01:26,  3.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▌  | 826/1096 [08:12<01:26,  3.11it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 75%|███████▌  | 827/1096 [08:13<01:30,  2.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▌  | 828/1096 [08:13<01:28,  3.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▌  | 829/1096 [08:13<01:27,  3.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▌  | 830/1096 [08:14<01:21,  3.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▌  | 831/1096 [08:14<01:21,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▌  | 832/1096 [08:14<01:14,  3.56it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▌  | 833/1096 [08:14<01:17,  3.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▌  | 834/1096 [08:15<01:18,  3.33it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▌  | 835/1096 [08:15<01:18,  3.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▋  | 836/1096 [08:15<01:27,  2.98it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▋  | 837/1096 [08:16<01:21,  3.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 76%|███████▋  | 838/1096 [08:16<01:24,  3.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 839/1096 [08:16<01:30,  2.85it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 840/1096 [08:17<01:19,  3.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 841/1096 [08:17<01:12,  3.52it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 842/1096 [08:17<01:06,  3.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 843/1096 [08:17<01:11,  3.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 844/1096 [08:18<01:14,  3.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 845/1096 [08:18<01:18,  3.22it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 846/1096 [08:18<01:22,  3.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 847/1096 [08:19<01:18,  3.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 848/1096 [08:19<01:15,  3.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 77%|███████▋  | 849/1096 [08:19<01:19,  3.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 850/1096 [08:20<01:19,  3.09it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 851/1096 [08:20<01:24,  2.91it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 852/1096 [08:20<01:15,  3.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 853/1096 [08:21<01:15,  3.23it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 854/1096 [08:21<01:24,  2.87it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 855/1096 [08:21<01:19,  3.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 856/1096 [08:22<01:20,  3.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 857/1096 [08:22<01:17,  3.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 858/1096 [08:22<01:19,  2.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 859/1096 [08:23<01:17,  3.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 78%|███████▊  | 860/1096 [08:23<01:18,  2.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▊  | 861/1096 [08:23<01:23,  2.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▊  | 862/1096 [08:24<01:21,  2.88it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▊  | 863/1096 [08:24<01:16,  3.04it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▉  | 864/1096 [08:24<01:10,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▉  | 865/1096 [08:25<01:08,  3.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▉  | 866/1096 [08:25<01:09,  3.33it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▉  | 867/1096 [08:25<01:12,  3.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▉  | 868/1096 [08:26<01:11,  3.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▉  | 869/1096 [08:26<01:14,  3.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▉  | 870/1096 [08:26<01:15,  3.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 79%|███████▉  | 871/1096 [08:27<01:17,  2.92it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|███████▉  | 872/1096 [08:27<01:13,  3.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|███████▉  | 873/1096 [08:27<01:14,  2.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|███████▉  | 874/1096 [08:28<01:10,  3.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|███████▉  | 875/1096 [08:28<01:05,  3.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|███████▉  | 876/1096 [08:28<01:09,  3.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|████████  | 877/1096 [08:29<01:13,  2.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|████████  | 878/1096 [08:29<01:13,  2.95it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|████████  | 879/1096 [08:29<01:12,  2.98it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|████████  | 880/1096 [08:30<01:10,  3.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|████████  | 881/1096 [08:30<01:07,  3.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 80%|████████  | 882/1096 [08:30<01:10,  3.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████  | 883/1096 [08:30<01:01,  3.46it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████  | 884/1096 [08:31<01:02,  3.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████  | 885/1096 [08:31<01:07,  3.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████  | 886/1096 [08:31<01:10,  2.99it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████  | 887/1096 [08:32<01:13,  2.84it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████  | 888/1096 [08:32<01:11,  2.92it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████  | 889/1096 [08:33<01:13,  2.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████  | 890/1096 [08:33<01:08,  3.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████▏ | 891/1096 [08:33<01:07,  3.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████▏ | 892/1096 [08:33<00:57,  3.52it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 81%|████████▏ | 893/1096 [08:34<01:00,  3.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 894/1096 [08:34<01:01,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 895/1096 [08:34<01:01,  3.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 896/1096 [08:35<01:02,  3.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 897/1096 [08:35<00:55,  3.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 898/1096 [08:35<00:55,  3.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 899/1096 [08:35<00:53,  3.68it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 900/1096 [08:36<00:56,  3.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 901/1096 [08:36<01:00,  3.22it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 902/1096 [08:36<01:01,  3.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 903/1096 [08:37<00:59,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 82%|████████▏ | 904/1096 [08:37<01:00,  3.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 905/1096 [08:37<00:59,  3.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 906/1096 [08:38<00:58,  3.24it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 907/1096 [08:38<00:56,  3.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 908/1096 [08:38<00:58,  3.24it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 909/1096 [08:38<00:50,  3.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 910/1096 [08:39<00:52,  3.52it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 911/1096 [08:39<00:58,  3.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 912/1096 [08:39<00:57,  3.22it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 913/1096 [08:40<00:56,  3.23it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 914/1096 [08:40<00:53,  3.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 83%|████████▎ | 915/1096 [08:40<00:53,  3.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▎ | 916/1096 [08:41<00:51,  3.49it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▎ | 917/1096 [08:41<00:53,  3.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▍ | 918/1096 [08:41<00:53,  3.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▍ | 919/1096 [08:41<00:52,  3.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▍ | 920/1096 [08:42<00:53,  3.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▍ | 921/1096 [08:42<00:53,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▍ | 922/1096 [08:42<00:53,  3.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▍ | 923/1096 [08:43<00:53,  3.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▍ | 924/1096 [08:43<00:52,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▍ | 925/1096 [08:43<00:52,  3.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 84%|████████▍ | 926/1096 [08:44<00:52,  3.22it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▍ | 927/1096 [08:44<00:53,  3.15it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▍ | 928/1096 [08:44<00:57,  2.91it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▍ | 929/1096 [08:45<00:53,  3.13it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▍ | 930/1096 [08:45<00:55,  3.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▍ | 931/1096 [08:45<00:52,  3.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▌ | 932/1096 [08:46<00:49,  3.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▌ | 933/1096 [08:46<00:48,  3.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▌ | 934/1096 [08:46<00:51,  3.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▌ | 935/1096 [08:47<00:52,  3.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▌ | 936/1096 [08:47<00:52,  3.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 85%|████████▌ | 937/1096 [08:47<00:52,  3.00it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▌ | 938/1096 [08:48<00:53,  2.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▌ | 939/1096 [08:48<00:53,  2.94it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▌ | 940/1096 [08:48<00:55,  2.79it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▌ | 941/1096 [08:49<00:56,  2.76it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▌ | 942/1096 [08:49<00:52,  2.93it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▌ | 943/1096 [08:49<00:50,  3.01it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▌ | 944/1096 [08:50<00:50,  3.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▌ | 945/1096 [08:50<00:44,  3.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▋ | 946/1096 [08:50<00:47,  3.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▋ | 947/1096 [08:51<00:49,  3.03it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 86%|████████▋ | 948/1096 [08:51<00:49,  3.02it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 949/1096 [08:51<00:48,  3.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 950/1096 [08:51<00:46,  3.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 951/1096 [08:52<00:47,  3.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 952/1096 [08:52<00:43,  3.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 953/1096 [08:52<00:45,  3.16it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 954/1096 [08:53<00:46,  3.05it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 955/1096 [08:53<00:41,  3.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 956/1096 [08:53<00:42,  3.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 957/1096 [08:54<00:43,  3.21it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 87%|████████▋ | 958/1096 [08:54<00:43,  3.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 959/1096 [08:54<00:44,  3.10it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 960/1096 [08:55<00:44,  3.07it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 961/1096 [08:55<00:40,  3.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 962/1096 [08:55<00:39,  3.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 963/1096 [08:55<00:40,  3.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 964/1096 [08:56<00:39,  3.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 965/1096 [08:56<00:39,  3.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 966/1096 [08:56<00:38,  3.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 967/1096 [08:57<00:39,  3.23it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 968/1096 [08:57<00:38,  3.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 88%|████████▊ | 969/1096 [08:57<00:36,  3.52it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▊ | 970/1096 [08:58<00:37,  3.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▊ | 971/1096 [08:58<00:39,  3.20it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▊ | 972/1096 [08:58<00:39,  3.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▉ | 973/1096 [08:59<00:40,  3.06it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▉ | 974/1096 [08:59<00:37,  3.23it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▉ | 975/1096 [08:59<00:36,  3.28it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▉ | 976/1096 [08:59<00:36,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▉ | 977/1096 [09:00<00:37,  3.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▉ | 978/1096 [09:00<00:34,  3.47it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▉ | 979/1096 [09:00<00:33,  3.47it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 89%|████████▉ | 980/1096 [09:01<00:33,  3.47it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|████████▉ | 981/1096 [09:01<00:33,  3.47it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|████████▉ | 982/1096 [09:01<00:32,  3.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|████████▉ | 983/1096 [09:01<00:33,  3.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|████████▉ | 984/1096 [09:02<00:33,  3.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|████████▉ | 985/1096 [09:02<00:34,  3.25it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|████████▉ | 986/1096 [09:02<00:33,  3.29it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|█████████ | 987/1096 [09:03<00:32,  3.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|█████████ | 988/1096 [09:03<00:32,  3.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|█████████ | 989/1096 [09:03<00:31,  3.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|█████████ | 990/1096 [09:04<00:32,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 90%|█████████ | 991/1096 [09:04<00:31,  3.35it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████ | 992/1096 [09:04<00:31,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████ | 993/1096 [09:05<00:32,  3.12it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████ | 994/1096 [09:05<00:29,  3.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████ | 995/1096 [09:05<00:31,  3.26it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████ | 996/1096 [09:05<00:31,  3.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████ | 997/1096 [09:06<00:27,  3.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████ | 998/1096 [09:06<00:28,  3.42it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████ | 999/1096 [09:06<00:28,  3.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████ | 1000/1096 [09:07<00:27,  3.53it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████▏| 1001/1096 [09:07<00:25,  3.65it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 91%|█████████▏| 1002/1096 [09:07<00:24,  3.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1003/1096 [09:07<00:26,  3.52it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1004/1096 [09:08<00:26,  3.42it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1005/1096 [09:08<00:27,  3.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1006/1096 [09:08<00:28,  3.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1007/1096 [09:09<00:26,  3.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1008/1096 [09:09<00:26,  3.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1009/1096 [09:09<00:25,  3.42it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1010/1096 [09:10<00:25,  3.36it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1011/1096 [09:10<00:26,  3.23it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1012/1096 [09:10<00:25,  3.33it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 92%|█████████▏| 1013/1096 [09:10<00:26,  3.14it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1014/1096 [09:11<00:26,  3.04it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1015/1096 [09:11<00:25,  3.18it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1016/1096 [09:11<00:24,  3.24it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1017/1096 [09:12<00:22,  3.49it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1018/1096 [09:12<00:21,  3.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1019/1096 [09:12<00:21,  3.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1020/1096 [09:13<00:22,  3.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1021/1096 [09:13<00:22,  3.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1022/1096 [09:13<00:22,  3.34it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1023/1096 [09:13<00:21,  3.39it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 93%|█████████▎| 1024/1096 [09:14<00:20,  3.56it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▎| 1025/1096 [09:14<00:19,  3.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▎| 1026/1096 [09:14<00:21,  3.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▎| 1027/1096 [09:15<00:21,  3.17it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▍| 1028/1096 [09:15<00:20,  3.27it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▍| 1029/1096 [09:15<00:18,  3.56it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▍| 1030/1096 [09:15<00:18,  3.63it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▍| 1031/1096 [09:16<00:17,  3.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▍| 1032/1096 [09:16<00:18,  3.50it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▍| 1033/1096 [09:16<00:17,  3.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▍| 1034/1096 [09:17<00:18,  3.38it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 94%|█████████▍| 1035/1096 [09:17<00:18,  3.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▍| 1036/1096 [09:17<00:16,  3.60it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▍| 1037/1096 [09:17<00:16,  3.65it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▍| 1038/1096 [09:18<00:16,  3.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▍| 1039/1096 [09:18<00:15,  3.64it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▍| 1040/1096 [09:18<00:16,  3.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▍| 1041/1096 [09:19<00:16,  3.35it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▌| 1042/1096 [09:19<00:16,  3.33it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▌| 1043/1096 [09:19<00:16,  3.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▌| 1044/1096 [09:19<00:14,  3.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▌| 1045/1096 [09:20<00:14,  3.61it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 95%|█████████▌| 1046/1096 [09:20<00:13,  3.63it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▌| 1047/1096 [09:20<00:13,  3.55it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▌| 1048/1096 [09:21<00:13,  3.57it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▌| 1049/1096 [09:21<00:13,  3.53it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▌| 1050/1096 [09:21<00:13,  3.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▌| 1051/1096 [09:21<00:13,  3.31it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▌| 1052/1096 [09:22<00:12,  3.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▌| 1053/1096 [09:22<00:12,  3.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▌| 1054/1096 [09:22<00:12,  3.32it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▋| 1055/1096 [09:23<00:12,  3.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▋| 1056/1096 [09:23<00:11,  3.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 96%|█████████▋| 1057/1096 [09:23<00:11,  3.41it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1058/1096 [09:24<00:11,  3.44it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1059/1096 [09:24<00:10,  3.58it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1060/1096 [09:24<00:10,  3.29it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1061/1096 [09:24<00:09,  3.66it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1062/1096 [09:25<00:09,  3.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1063/1096 [09:25<00:08,  3.67it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1064/1096 [09:25<00:08,  3.90it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1065/1096 [09:25<00:08,  3.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1066/1096 [09:26<00:08,  3.75it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1067/1096 [09:26<00:06,  4.19it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 97%|█████████▋| 1068/1096 [09:26<00:06,  4.46it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1069/1096 [09:26<00:06,  4.30it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1070/1096 [09:27<00:06,  3.96it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1071/1096 [09:27<00:06,  3.76it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1072/1096 [09:27<00:06,  3.59it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1073/1096 [09:27<00:06,  3.37it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1074/1096 [09:28<00:06,  3.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1075/1096 [09:28<00:06,  3.43it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1076/1096 [09:28<00:05,  3.45it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1077/1096 [09:29<00:05,  3.40it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1078/1096 [09:29<00:04,  3.69it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 98%|█████████▊| 1079/1096 [09:29<00:04,  3.63it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▊| 1080/1096 [09:29<00:04,  3.51it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▊| 1081/1096 [09:30<00:03,  3.89it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▊| 1082/1096 [09:30<00:03,  3.78it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▉| 1083/1096 [09:30<00:03,  3.93it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▉| 1084/1096 [09:30<00:03,  3.74it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▉| 1085/1096 [09:31<00:02,  3.74it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▉| 1086/1096 [09:31<00:02,  3.56it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▉| 1087/1096 [09:31<00:02,  3.84it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▉| 1088/1096 [09:32<00:02,  3.85it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▉| 1089/1096 [09:32<00:01,  3.81it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"," 99%|█████████▉| 1090/1096 [09:32<00:01,  3.54it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|█████████▉| 1091/1096 [09:32<00:01,  3.59it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|█████████▉| 1092/1096 [09:33<00:01,  3.71it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|█████████▉| 1093/1096 [09:33<00:00,  3.70it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|█████████▉| 1094/1096 [09:33<00:00,  3.64it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|█████████▉| 1095/1096 [09:33<00:00,  3.62it/s]/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","100%|██████████| 1096/1096 [09:34<00:00,  1.91it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"K-vciGFWKyGx"},"source":["scores = np.array(scores)\n","with open('/content/drive/My Drive/Rebuild my Professor/scores.npy', 'wb') as f:\n","  np.save(f, scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":733},"id":"eJ5zNc7begEN","executionInfo":{"status":"ok","timestamp":1617953337156,"user_tz":300,"elapsed":1369,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"a0aa009a-809f-4eea-830f-8120e4c15ed5"},"source":["import matplotlib as mpl\n","%matplotlib inline\n","fig, axes = plt.subplots(5, 1, figsize=(8, 12))\n","wspace = 0.2 \n","hspace = 0.6\n","plt.subplots_adjust(wspace=wspace, hspace=hspace)\n","sns.histplot(x=scores[:, 1], ax=axes[0])\n","sns.histplot(x=scores[:, 2], ax=axes[1])\n","sns.histplot(x=scores[:, 3], ax=axes[2])\n","sns.histplot(x=scores[:, 4], ax=axes[3])\n","sns.histplot(x=scores[:, 5], ax=axes[4])\n","axes[0].set_title(\"Participation\")\n","axes[1].set_title(\"Bonus\")\n","axes[2].set_title(\"Engaging Lecture\")\n","axes[3].set_title(\"Office Hour\")\n","axes[4].set_title(\"Workload\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'Workload')"]},"metadata":{"tags":[]},"execution_count":156},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x864 with 5 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"gvBsDu2fgLUe"},"source":["with open('/content/drive/My Drive/Rebuild my Professor/scores.npy', 'rb') as f:\n","  scores = np.load(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WnH0CQo1aj3z","executionInfo":{"status":"ok","timestamp":1618086342896,"user_tz":300,"elapsed":311,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"c58df929-2165-4a3d-d534-ce2b868c57d6"},"source":["model.eval()\n","\n","sentence = \"He is fantastic\"\n","sentence = \" \".join(sentence.split())\n","inputs = tokenizer.encode_plus(\n","    sentence,\n","    None,\n","    add_special_tokens=True,\n","    max_length=MAX_LEN,\n","    pad_to_max_length=True,\n","    return_token_type_ids=True,\n","    truncation=True\n",")\n","ids = inputs['input_ids']\n","mask = inputs['attention_mask']\n","\n","ids = torch.tensor(ids, dtype=torch.long).to(device, dtype=torch.long).unsqueeze(0)\n","mask = torch.tensor(mask, dtype=torch.long).to(device, dtype=torch.long).unsqueeze(0)\n","outputs = model(ids, mask).squeeze()\n","outputs = outputs.cpu().detach().numpy()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"3iKEQevAebDS"},"source":["prof_scores = np.array(scores)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pv0UyKV6dSwJ","executionInfo":{"status":"ok","timestamp":1618087036978,"user_tz":300,"elapsed":339,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"871acba3-3fb6-443f-d7fa-d29c88b6367d"},"source":["0.2 + (1-0.2) * (sum(scores[:, 1] < prof_scores[1]) / scores.shape[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6343065693430657"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HFcGxW6djty","executionInfo":{"status":"ok","timestamp":1618087006168,"user_tz":300,"elapsed":278,"user":{"displayName":"Ao Qu","photoUrl":"","userId":"08921636041567139920"}},"outputId":"35fb807c-98be-499e-b427-2fb9214354d0"},"source":["scores[:, 1] < prof_scores[1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([False,  True,  True, ...,  True, False, False])"]},"metadata":{"tags":[]},"execution_count":34}]}]}